---
title: "Davis Hackathon 2022-Visualizing Flashes in Video"
excerpt_separator: "<!--more-->"
categories:
  - Blog
tags:
  - Technology
  - Technical Writing
author_profile: false
---
<h1 id="Davis-Hackathon-2022-Visualizing-Flashes-in-Video">Davis Hackathon 2022-Visualizing Flashes in Video<a class="anchor-link" href="#Davis-Hackathon-2022-Visualizing-Flashes-in-Video">&#182;</a></h1>
<p>In spring of 2022 I participated in a hackathon at UC Davis. I collaborated with Kay Krachenfels, Omar Khan, and Michelle Lu to begin work on an application that inserts photo-sensitivity warnings into video. During the 24 hours of the hackathon, we visualized where flashes happened in videos and created a web page where users could upload video files for processing. 24 hours was not enough time to engineer a way to accurately make predictions, but I am proud of what we accomplished.</p>

<h2 id="Project-Goals">Project Goals<a class="anchor-link" href="#Project-Goals">&#182;</a></h2>
<p>Many companies insert photo-sensitivity warnings into the beginning of content that has flashes. However, people with photosensitive epilepsy still face challenges consuming media. Most videos that have photosensitivity warnings only have a general warning and do not help viewers avoid flashing lights. Having optional indicators before and after scenes with flashing lights would help people with photosensitive epilepsy take precautions like turning lights on in the room or looking away from the screen.</p>
<p>There are already tools that can detect flashes in video and web content, but our tool focuses on inserting automated warnings. The goal of the project during hackathon was to visualize where flashes happen in videos and design a website that users could upload videos to. The short-term goal for this project is to create a web site where users can upload videos and download a copy of the video with photosensitivity warnings at specific times stamps. The long-term goal is to develop a tool that can insert photosensitivity warnings into streaming services. This tool would be valuable for companies like Netflix, Youtube, and Twitch.</p>

<h2 id="My-Role">My Role<a class="anchor-link" href="#My-Role">&#182;</a></h2>
<p>I processed video into a two-dimensional representation of changes in luminance and visualized that data to identify where flashes happen.</p>

<h2 id="Preprocessing">Preprocessing<a class="anchor-link" href="#Preprocessing">&#182;</a></h2>
<p>The first step towards making predictions is transforming the video into a simpler format. Videos are an example of 4D Tensors or in other words an array with four axes:
    •   Frames
    •   Height of frame
    •   Width of frame
    •   Color
    I decided that reducing the data into a 2D array would be the best way to identify flashes.</p>

<h3 id="The-getvideo-function">The getvideo function<a class="anchor-link" href="#The-getvideo-function">&#182;</a></h3><p>the getvideo() function processes the video:</p>
<ol>
<li>Saves each frame of the video as an RGB image.</li>
<li>Converts each image into grayscale.</li>
<li>Takes the average grayscale value of regions of the frame.</li>
<li>Stores each grayscale value in a 2D array where each row is a region of the screen and each column is a frame.</li>
</ol>
<pre>
    <code>
        def getvideo(vid):
    vidcap = cv2.VideoCapture(vid)
    
    success, image = vidcap.read()

    count = 1

    avg_gray_over_time = []

    while success:
        imgGray = color.rgb2gray(image)
        
        #print('manipulated frame', count)

        success, image = vidcap.read()
        count+=1 
        

        avg_gray = []
        
        first_split = np.vsplit(imgGray, 3)
        for segment in first_split:
            second_split = np.hsplit(segment, 4)
            for small_seg in second_split:
                avg_gray.append(np.average(small_seg))

        avg_gray_over_time.append(avg_gray)


    #print(avg_gray_over_time)
    print("Manipulated ", count, " frames")

    #convert list to numpy array
    gray_vs_time = np.array(avg_gray_over_time)
    return gray_vs_time
    </code>
</pre>

<h4 id="Why-grayscale-values-of-screen-regions-over-time?">Why grayscale values of screen regions over time?<a class="anchor-link" href="#Why-grayscale-values-of-screen-regions-over-time?">&#182;</a></h4>
<p>You might be wondering why I convert the video into a single grayscale image that tracks changes in 12 regions of the screen instead of working with a vector of grayscale images or some other less abstract representation.</p>
<p>As an accessibility tester I test web sites to see if they meet web accessibility standards. WCAG defines a general flash threshold to determine if content contains flashes that pose a risk for people with photosensitive epilepsy. A flash is dangerous when there are more than three flashes within one second and the combined area of the flashes take up more than 25% of the screen. An individual flash is a pair of opposing changes in relative luminance more than 10% of the maximum relative luminance. Luminance is a grayscale value. To find luminance I use OpenCV to convert RGB into a value between zero and one.</p>
<p>Segmenting the screen into regions makes it easier to identify when more than 25% of the screen is flashing. If 3 or more of the segment’s luminance changes above the threshold defined by WCAG 3 times in 1 second, it is a flash. Saving it as a single image makes it simpler to access specific time stamps and apply machine learning methods to them.</p>

<h2>Visualization</h2>
<p>I use matplotlib.imshow to visualize the transformed video. I plot the entire video, 25 frames of video, and then 25 frames with different thresholds that show when luminance changes by more than a specific amount. Plotting changes above a specific threshold gives an idea of where the flashes in the video are.</p>

<p>While visualizing the data I took inspiration from spectrograms used in speech recognition systems. A spectrogram is a 2D matrix where each row is a frequency, each column is a timestamp, and each data point is the magnitude of the frequency. Speech recognition systems will take small sections of these images 20 ms to 30 ms and overlap them to extract features in the signal. For detecting flashes, I need to work with sections that are around one second long. The video visualized below is 25 FPS. I am working on identifying where the flashes are by using overlapping 25 frame segments. </p>

<p>I tried visualizing three different kinds of video: Partial flashes on a black background, flashes in an animated video, and flashes in a live action video.</p>
<h3>Partial flashes on a black background </h3>
<figure>
    <figcaption>Partial flashes on a black background</figcaption>
    <image src='/assets/images/partflash.png'>
    </image>
</figure>
<figure>
    <figcaption>Partial flashes on a black background graphed as changes in luminance:</figcaption>
    <image src='/assets/images/partflashfull.png'>
    </image>
</figure>
<figure>
    <figcaption>Partial flashes on a black background 25 frames frame slice</figcaption>
    <image src='/assets/images/partflash25.png'>
    </image>
</figure>
<figure>
    <figcaption>Partial flashes on a black background with a change in luminance over 0.3</figcaption>
    <image src='/assets/images/partflash03.png'>
    </image>
</figure>


<h3>Flashes in a cartoon</h3>

<figure>
    <figcaption>An example of a flash in an episode of Pokémon:</figcaption>
    <image src='/assets/images/pory.png'>
    </image>
</figure>

<figure>
    <figcaption>Flashes in an episode of Pokémon graphed as changes in luminance</figcaption>
    <image src='/assets/images/poryfull.png'>
    </image>
</figure>

<figure>
    <figcaption>Flashes in an episode of Pokémon 25 frame slice</figcaption>
    <image src='/assets/images/pory25.png'>
    </image>
</figure>

<figure>
    <figcaption>Flashes in an episode of Pokémon With the change in luminance over .7</figcaption>
    <image src='/assets/images/pory07.png'>
    </image>
</figure>

<h3>Flashes in live action video:</h3>

<figure>
    <figcaption>An example of a flash in an Alien movie</figcaption>
    <image src='/assets/images/alien.png'>
    </image>
</figure>

<figure>
    <figcaption>Flashes in an alien movie graphed as changes in luminance:</figcaption>
    <image src='/assets/images/livefull.png'>
    </image>
</figure>

<figure>
    <figcaption>Flashes in an alien movie 200 frame slice</figcaption>
    <image src='/assets/images/live25.png'>
    </image>
</figure>
<figure>
    <figcaption>Flashes in an alien movie With changes in luminance over .4</figcaption>
    <image src='/assets/images/live04.png'>
    </image>
</figure>
